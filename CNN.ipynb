{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import os\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'archive/seg_train/seg_train/'\n",
    "test_path = 'archive/seg_test/seg_test/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add GPU if Available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "    print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (150, 150) # 150x150 pixels\n",
    "\n",
    "def load_images(path):\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    folders = sorted(os.listdir(path), key=lambda x: os.path.getmtime(os.path.join(path, x)))\n",
    "\n",
    "    for i, folder in enumerate(folders):\n",
    "        for file in os.listdir(os.path.join(path, folder)):\n",
    "            img = cv2.imread(os.path.join(path, folder, file))\n",
    "            img = cv2.resize(img, image_size)\n",
    "            X.append(img)\n",
    "            y.append(i)\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_train, y_train = load_images(train_path)\n",
    "X_test, y_test = load_images(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the number if images in each class in the training and test data\n",
    "labels = {0: 'buildings', 1: 'forest', 2: 'glacier', 3: 'mountain', 4: 'sea', 5: 'street'}\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(*np.unique(y_train, return_counts=True), label='Training data', alpha=0.5)\n",
    "plt.bar(*np.unique(y_test, return_counts=True), label='Test data', alpha=0.5)\n",
    "plt.title('Number of Images in Each Class')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of Images')\n",
    "plt.xticks(range(len(labels)), labels.values())  # Set the x-axis tick labels\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display some images from the training data per class\n",
    "fig, axes = plt.subplots(1, 6, figsize=(20, 20))\n",
    "for i in range(6):\n",
    "    axes[i].imshow(X_train[y_train == i][0])\n",
    "    axes[i].set_title(labels[i])\n",
    "    axes[i].axis('off')\n",
    "plt.show()\n",
    "\n",
    "# display some images from the test data per class\n",
    "fig, axes = plt.subplots(1, 6, figsize=(20, 20))\n",
    "for i in range(6):\n",
    "    axes[i].imshow(X_test[y_test == i][0])\n",
    "    axes[i].set_title(labels[i])\n",
    "    axes[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Training and Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode the labels\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# split the training data into training and validation data\n",
    "# 80% of the data will be used for training and 20% for validation\n",
    "# split the data stratified by the labels\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal way to normalize (use this)\n",
    "X_train = X_train / 255.0\n",
    "X_val = X_val / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# # nomalize when pc memory is not enough (...)\n",
    "# def normalize_data(data):\n",
    "#     return data / 255.0\n",
    "\n",
    "# # get the shape of the data\n",
    "# print(X_train.shape, y_train.shape)\n",
    "# print(X_val.shape, y_val.shape)\n",
    "# print(X_test.shape, y_test.shape)\n",
    "\n",
    "# try:\n",
    "#     # Normalize training data\n",
    "#     chunk_size = 500  # Adjust based on memory capacity\n",
    "#     for i in range(0, len(X_train), chunk_size):\n",
    "#         X_train[i:i+chunk_size] = normalize_data(X_train[i:i+chunk_size])\n",
    "    \n",
    "#     # Normalize validation data\n",
    "#     for i in range(0, len(X_val), chunk_size):\n",
    "#         X_val[i:i+chunk_size] = normalize_data(X_val[i:i+chunk_size])\n",
    "    \n",
    "#     # Normalize test data\n",
    "#     for i in range(0, len(X_test), chunk_size):\n",
    "#         X_test[i:i+chunk_size] = normalize_data(X_test[i:i+chunk_size])\n",
    "\n",
    "#     print(\"Normalization completed successfully.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"An error occurred during normalization: {e}\")\n",
    "\n",
    "# print(X_train.shape, y_train.shape)\n",
    "# print(X_val.shape, y_val.shape)\n",
    "# print(X_test.shape, y_test.shape)\n",
    "\n",
    "# # check if the data is normalized\n",
    "# print(X_train[0].max(), X_train[0].min())\n",
    "# print(X_val[0].max(), X_val[0].min())\n",
    "# print(X_test[0].max(), X_test[0].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the accuracy of the model\n",
    "_, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "_, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "_, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print('Train accuracy:', train_acc)\n",
    "print('Validation accuracy:', val_acc)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('cnn1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(128, (5, 5), activation='relu', input_shape=(150, 150, 3), padding='same'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=np.sqrt(0.1), patience=5)\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=(X_val, y_val), callbacks=[reduce_lr], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the accuracy of the model\n",
    "_, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "_, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "_, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print('Train accuracy:', train_acc)\n",
    "print('Validation accuracy:', val_acc)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('cnn2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(128, (5, 5), activation='relu', input_shape=(150, 150, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2), padding='same'))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2), padding='same'))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2), padding='same'))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=np.sqrt(0.1), patience=5)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_whitening=False,  # apply ZCA whitening\n",
    "    rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    zoom_range = 0.1, # Randomly zoom image\n",
    "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=False,  # randomly flip images\n",
    "    vertical_flip=False # randomly flip images\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', patience=2, verbose=1, factor=0.5, min_lr=0.00001)\n",
    "\n",
    "history = model.fit(datagen.flow(X_train, y_train, batch_size=128), epochs=20, validation_data=(X_val, y_val), callbacks=[reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the accuracy of the model\n",
    "_, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "_, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "_, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print('Train accuracy:', train_acc)\n",
    "print('Validation accuracy:', val_acc)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('cnn3.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
